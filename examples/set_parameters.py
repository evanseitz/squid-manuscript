# =============================================================================
# Customizable hyperparameters for the SQUID pipeline are contained in the four..
# ..functions below: 'set_params_1()' through 'set_params_4()', each corresponding to..
# ..the Python script with identical index: e.g., 'set_params_1()' contains parameters..
# ..required in '1_located_patterns.py'. As subsequent scripts will load in all..
# ..parameters from preceding steps, these parameters should not be changed across..
# ..the SQUID pipeline for any given analysis. Instructions for choosing these..
# ..parameters are contained in each respective Python script, with more detailed..
# ..information about each variable provided in the comments below. Specific choices..
# ..for parameters used in our demonstration are provided at the bottom of this script
# =============================================================================
# Instructions: Parameter choices should be made in order of index for the four..
#               ..functions below. In the first function, 'set_params_1()', the..
#               ..most broad information for a desired analysis is defined, such..
#               ..as the choice of example deep learning model and the kind..
#               ..of recognition site(s) to explore using our surrogate modeling..
#               ..framework. Subsequent functions contain options for defining..
#               ..more fine-grained parameters used in later stepts. Once these..
#               ..parameters are defined, proceed to running the first of the..
#               ..four Python scripts in our pipeline, as described in the header..
#               ..of 'python 1_locate_patterns.py'
# =============================================================================

import os, sys, time
sys.dont_write_bytecode = True
import warnings
warnings.filterwarnings("ignore", message=r"Passing", category=FutureWarning)
import numpy as np

import tensorflow as tf
from keras import backend as K


def get_prediction(OH, example, model):
    """
    Customizable function required for retrieving a specific deep learning model prediction
    ----------
    OH : ARRAY with shape (L, 4)
        Input sequence (one-hot encoding)
    example : STRING {'GOPHER', 'DeepSTARR', 'BPNet', 'TFChIP' or 'custom'}
        Name of example pipeline to follow to reproduce our published results..
        ..for a specific deep learning model (or define a new deep learning model)
    model : FUNCTION (model dependent)
        The user-defined deep learning model

    Returns
    -------
    pred : ARRAY or DICT (model dependent)
        Deep learning model prediction corresponding to the input one-hot encoding
    """
    if example == 'GOPHER' or example == 'CAGI5-GOPHER':
        pred = model.predict(OH)
        
    elif example == 'BPNet':
        pred = model.predict(OH)
        
    elif example == 'DeepSTARR':
        if 0: #kipoi model
            pred = model.predict_on_batch(OH)
        else: #keras model from https://github.com/bernardo-de-almeida/DeepSTARR
            pred = model.predict(OH)
            
    elif example == 'TFChIP':
        import tensorflow as tf
        logit = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-2].output)
        logit_pred = logit.predict(OH)
        pred = logit_pred[:,0]

    elif example == 'CAGI5-ENFORMER':
        pred = model.predict_on_batch(OH)['human'] #options: {'human' or 'mouse'}
        pred = np.array(pred).astype(np.float32)
        
    return pred


def unwrap_prediction(pred, class_idx, pred_n, example, pred_transform):
    """
    Customizable function required for unwrapping a specific deep learning model prediction
    ----------
    pred : ARRAY or DICT etc (model dependent)
        Deep learning model prediction output based on a given input sequence
    class_idx : INT or STRING (model dependent)
        The indexing scheme for obtaining a desired prediction property from the deep learning model
    pred_n : INT
        Index of prediction if contained in an array; else zero 
    example : STRING {'GOPHER', 'DeepSTARR', 'BPNet', 'TFChIP' or 'custom'}
        Name of example pipeline to follow to reproduce our published results..
        ..for a specific deep learning model (or define a new deep learning model)

    Returns
    -------
    unwrap_pred : ARRAY (model dependent)
        Isolated prediction corresponding to a desired property generated by the deep learning model
    """
    
    if example == 'GOPHER':
        unwrap_pred = pred[pred_n,:,class_idx]

    elif example == 'CAGI5-GOPHER':
        unwrap_pred = np.mean(pred[pred_n,:,:], axis=1)
        
    elif example == 'BPNet':
        if pred_transform != 'wn':
            strand = 'positive' #STRING {'positive' or 'negative'}
            if strand == 'positive':
                unwrap_pred = pred[class_idx][pred_n][:,0]
            elif strand == 'negative':
                unwrap_pred = pred[class_idx][pred_n][:,1]
        else:
            unwrap_pred = pred[class_idx][pred_n]
            
    elif example == 'DeepSTARR':
        unwrap_pred = float(pred[class_idx][pred_n])
        
    elif example == 'TFChIP':
        unwrap_pred = float(pred[pred_n])

    elif example == 'CAGI5-ENFORMER':
        unwrap_pred = pred[pred_n]
        unwrap_pred = np.mean(unwrap_pred[:,class_idx], axis=1)
        
    elif example == 'custom':
        print('Write custom function here.')
        
    else:
        print("Error: user input '%s' is not recognized." % example)
        
    return unwrap_pred


def compress_prediction(pred, pred_transform, pred_trans_delimit, delimit_start=None, delimit_stop=None):
    """
    Customizable function required for compressing a targeted deep learning model prediction into..
    a single scalar value. Targeted model predictions are first isolated via 'unwrap_predictions()'
    ----------
    pred : ARRAY
        Targeted deep learning model prediction obtained by 'unwrap_predictions()'
    pred_transform : STRING {'sum', 'max', 'pca', 'custom' or None}
        'sum':      Scalar y is the integral over the prediction
        'max':      Scalar y is the max value of the prediction
        'pca':      Scalar y is defined by the first principal component; generated by PCA over all predictions
        'custom':   Users may also define their own function below
        None:       Set to None if model predictions are already in single-value (scalar) format per sequence
    pred_trans_delimit : INT >= 0 or None
        Delimit range of values taken during above transformation; e.g., if 'sum' is chosen above,
        a value of 5 will sum over five flanking units on each side of the sequence's center position.
        Units correspond to the prediction output, not the input (e.g., bins, not base pairs)
    delimit_start : INT
        Specify start position for range of 'pred_trans_delimit'
    delimit_stop : INT
        Specify stop position for range of 'pred_trans_delimit'
    Returns
    -------
    score : SCALAR (model dependent)
        Compressed prediction corresponding to a desired property generated by the deep learning model
    """

    if pred_trans_delimit is not None:
        if delimit_start or delimit_stop is None:
            pred_in = int((pred.shape[0]/2)-pred_trans_delimit)
            pred_out = int((pred.shape[0]/2)+pred_trans_delimit)
        else:
            pred_in = delimit_start
            pred_out = delimit_stop
        pred = pred[pred_in:pred_out]

    if pred_transform == 'sum':
        score = sum(pred)

    elif pred_transform == 'max':
        score = np.amax(pred)

    elif pred_transform == 'pca':
        print('TBD') #for usage, see '2_generate_mave.py' and the pca() function in 'squid/utils.py'

    elif pred_transform == 'wn': #transformation used in the original BPNet paper
        #import keras.backend as K
        graph = tf.Graph()
        with graph.as_default():
            #wn = K.sum(K.stop_gradient(K.softmax(pred)) * pred, axis=-1)
            wn = tf.reduce_mean(tf.reduce_sum(tf.stop_gradient(tf.nn.softmax(pred)) * pred, axis=-2), axis=-1)
        with tf.Session(graph=graph).as_default() as sess:
            score = float(wn.eval())

    elif pred_transform == 'custom':
        print('Write custom function here.')

    elif pred_transform is None:
        score = pred

    else:
        print("Error: user input '%s' is not recognized." % pred_transform)
    
    return score
        

def set_params_1(pyDir, load_model):
    """
    Parameters required for '1_locate_patterns.py' and used in subsequent scripts
    ----------
    pyDir : STRING
        Path of the parent Python script
    load_model : BOOL {True or False}
        If True, load deep learning model along with user parameters
        If False, only load user parameters

    Returns
    -------
    GPU : BOOL {True or False}
        If True, certain algorithms will be run in modes more favorable to GPUs
    example : STRING {'GOPHER', 'DeepSTARR', 'BPNet' or 'custom'}
        Name of example pipeline to follow to reproduce our published results..
        ..for a specific deep learning model (or define a new deep learning model)
        Using a given model, new recogntion sites can also be defined for new analysis..
        extending beyond the scope of our published work
    motif_A : STRING
        Sequence of main recognition site to search for and later analyze. Use 'N' to represent..
        less strict nucleotides (e.g., for optimal AP-1 core: 'TGANTCA')
    motif_B : STRING
        Sequence of secondary recognition site to search for if modeling inter-motif relationships..
        Use same naming rules as for motif_A. Set to None if performing surrogate modeling only on..
        ..single-motif instances (see use of 'intra'/'inter' strings below)
    motif_A_name : STRING
        Name of motif A to be used in filename of outputs (e.g., 'AP1')
    motif_B_name : STRING
        Name of motif B to be used in filename of outputs if 'motif_B' != None
    max_muts : INT
        Maximum number of core mutations required for putative recognition site to be recorded in dataframe
        All other instances above this number will be skipped during the search
    max_dist : INT
        If 'motif_B' != None, the maximum nearest-neighbor distance between 'motif_A' and 'motif_B'
        All other instances above this number will be skipped during the search
    rank_type : STRING {'ISM', 'saliency'} 
        Attribution score used to rank each 'motif_A' and 'motif_B' instance during the search
        In Silico Mutagenesis (ISM) is the gold-standard attribution-based approach; see Koo et al. 2021
    comparison_methods: ARRAY
        Attribution-based methods that are available for the current model to employ
        These methods are stored for a later (optional) comparison with surrogate modeling
        (e.g., ['ISM_single', 'ISM_double', 'saliency', 'deepLIFT', 'deepSHAP'])
        ISM_single:     In silico single mutagenesis
        ISM_double:     In silico double mutagenesis
            In silico mutagenesis is calculated by systematically querying a trained model with new sequences..
            ..with a different single nucleotide mutation along the sequence and ordering the predictions as a..
            ..nucleotide-resolution map (4 Ã— L, where 4 is for each nucleotide and L is the length of the sequence)
            Each prediction is subtracted by the wildtype sequence prediction, effectively giving zeros at..
            ..positions where the variant matches the wildtype sequence. To visualize the in silico mutagenesis..
            ..maps, a sequence logo is generated for the wildtype sequence, where heights correspond the..
            ..sensitivity of each position via the L2-norm across variants for each position (Koo et al., 2021)
        saliency:       Saliency maps
            Saliency maps are produced using first-order derivatives (one step of backpropagation in the DNN model)..
            ..to describe the importance of each nucleotide in making the final prediction. Following this, pointwise..
            ..multiplication of the saliency map with the one-hot encoded sequence is performed to get the derivative..
            ..values for the actual nucleotide characters of the sequence to show the influence of the character..
            ..at each position on the output score (Lanchantin et al., 2018)
        deepLIFT:       deepLIFT
            DeepLIFT (Deep Learning Important FeaTures) is a method for decomposing the output prediction of a neural..
            ..network on a specific input by backpropagating the contributions of all neurons in the network to every..
            ..feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and..
            ..assigns contribution scores according to the difference. By optionally giving separate consideration to..
            ..positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other..
            ..approaches. Scores can be computed efficiently in a single backward pass (Shrikumar et al., 2017)
        deepSHAP:       deepSHAP
            DeepSHAP (Deep SHapley Additive exPlanations) is a method for combining DeepLIFT and Shapley values
            (Scott Lundberg and Su-In Lee, 2017)
    model_name : STRING
        Choose between DNN architectures if more than one available for a given test set
    class_idx : INT or STRING (model dependent)
        Define the indexing scheme for obtaining a desired prediction property from the deep learning model
    log2FC : BOOL {True or False}
        If True, calculate ISM predictions via the log2 fold-change {log2(pred_var) - log2(pred_wt)}
        Additionally used during calculations for surrogate modeling
        If False, just take the difference {pred_var - pred_wt}
    alphabet : 1D ARRAY
        All characters present in the sequence alphabet (e.g., ['A','C','G','T'] for DNA)
    alpha : STRING {'dna','rna' or 'protein'}
        Naming convention of the above alphabet
    bin_res: INT >= 1
        Bin resolution for model predictions; e.g., {binning = 1} for base resolution
    output_skip: INT >= 0 (in units of final model output; e.g., bins)
        Required for models that output predictions with (non-binned) lengths less than the input sequence length;
        e.g., ENFORMER predicts the central 114688 bp region (aggregated into 128-bp bins) from a 393216 bp input
    model : FUNCTION (model dependent)
        The imported deep learning model based on the above parameters.
    X_in : TENSOR (shape=(N,L,A))
        One-hot encoding of sequence data to explore for surrogate modeling (e.g., obtained from test set)
        N:      Number of sequences
        L:      Length of each sequence
        A:      Length of alphabet
    """
    
    import logging
    import tensorflow as tf
    from tensorflow import keras
    # turn off tensorflow warnings:
    tf.get_logger().setLevel(logging.ERROR)
    logging.getLogger('tensorflow').disabled = True
    import h5py
    import zipfile
    import shutil
    
    GPU = False
    
    example = 'GOPHER' #{'GOPHER', 'DeepSTARR', 'BPNet', 'TFChIP', 'CAGI5-GOPHER', 'CAGI5-ENFORMER', 'custom'}
    max_muts = 0#2
    
    if example == 'GOPHER': #trained GOPHER deep learning model
        """
                User parameters for determing location of recognition site(s)
                Choose one via 'if 1:' and set all others to 'if 0:'
        """
        if 1:
            motif_A = 'TGAGTCA'
            motif_B = None
            motif_A_name = '13_AP1'
            motif_B_name = None
            max_dist = None
            class_idx = 13
            model_name = 'model_ResidualBind32_ReLU_single' #DNN model architecture
        if 0:
            motif_A = 'TGANTCA'
            motif_B = 'TGANTCA'
            motif_A_name = '13_AP1_N'
            motif_B_name = '13_AP1_N'
            max_dist = 20
            class_idx = 13
            model_name = 'model_ResidualBind32_Exp_single'
        if 0:
            motif_A = 'GGAAGT'
            motif_B = None
            motif_A_name = '7_SPI1'
            motif_B_name = None
            max_dist = None
            class_idx = 7
            model_name = 'model_ResidualBind32_ReLU_single'
        if 0:
            motif_A = 'CCGGAA' 
            motif_B = None
            motif_A_name = '9_FEV'
            motif_B_name = None
            max_dist = None
            class_idx = 9
            model_name = 'model_ResidualBind32_ReLU_single'
        if 0:
            motif_A = 'TGAAAC'
            motif_B = None
            motif_A_name = '7_IRF1'
            motif_B_name = None
            max_dist = None
            class_idx = 7
            model_name = 'model_ResidualBind32_ReLU_single'
        if 0:
            motif_A = 'AANTGAAAC'
            motif_B = None
            motif_A_name = '7_IRF1-long'
            motif_B_name = None
            max_dist = None
            class_idx = 7
            model_name = 'model_ResidualBind32_ReLU_single'
        if 0: #TO DO (X/100)
            motif_A = 'CTTCC'
            motif_B = None
            motif_A_name = '7_CTTCC'
            motif_B_name = None
            max_dist = None
            class_idx = 7
            model_name = 'model_ResidualBind32_ReLU_single'
            
        # ========================[HARDCODED MODEL PARAMETERS]======================== #    
        
        log2FC = False
        rank_type = 'saliency'
        comparison_methods = ['ISM_single', 'saliency']
        """ class_idx : INT in range [0,14]
                "GM21381" :     cell_line_0
                "GM23338" :     cell_line_1
                "HepG2" :       cell_line_2
                "RWPE2" :       cell_line_3
                "HG03575" :     cell_line_4
                "K562":         cell_line_5
                "DND-41" :      cell_line_6
                "GM12878" :     cell_line_7 -> 3458 sequences
                "A549" :        cell_line_8 -> 2740 sequences
                "HCT116" :      cell_line_9 -> 2297 sequences
                "IMR-90" :      cell_line_10
                "NCI-H929" :    cell_line_11
                "PANC1" :       cell_line_12
                "PC-3" :        cell_line_13
                "MCF-7" :       cell_line_14
            - citra: /shared/complete_zenodo/datasets/quantitative_data/cell_line_testsets/
            - comet: /shared/data00/complete_zenodo/datasets/quantitative_data/cell_line_testsets/
            - comet: /shared/data00/acme/data/atac/cell_line_testsets
        """
        alphabet = ['A','C','G','T']
        alpha = 'dna'
        bin_res = 32
        output_skip = 0
        
        userDir = os.path.join(pyDir, 'examples_%s' % example) #must already exist, with folder 'a_model_assets' propagated
        sys.path.append(os.path.join(userDir,'a_model_assets/scripts'))
        import saliency_embed, utils
    
        model_dir = os.path.join(userDir,'a_model_assets/%s' % model_name)
        if load_model is True: #retrieve trained deep learning model
            model, bin_size = utils.read_model(os.path.join(userDir,'a_model_assets/%s/' % model_name), compile_model = True)
        else:
            model = None
        """
        Inputs shape:   (n,2048,4)
        Outputs shape:  (1, 64, 15) : 64-bin-resolution profile for each of the 15 cell lines
        """
        # retrieve genomic sequences from test set
        with h5py.File(os.path.join(userDir, 'a_model_assets/cell_line_%s.h5' % class_idx), 'r') as dataset:
            X_in = np.array(dataset['X']).astype(np.float32)
        
        
    elif example == 'BPNet': #trained BPNet deep learning model
        """
                User parameters for determing location of recognition site(s)
                Choose one via 'if 1:' and set all others to 'if 0:'
        """
        if 0:
             motif_A = 'TTTGCAT'
             motif_B = None
             motif_A_name = 'Oct4'
             motif_B_name = None
             max_dist = None
             class_idx = 'Oct4/profile' #options: {'Oct4/profile', 'Sox2/profile', 'Klf4/profile' or 'Nanog/profile'}
        if 0:
             motif_A = 'TGCATATGCA'
             motif_B = None
             motif_A_name = 'Oct4-Oct4'
             motif_B_name = None
             max_dist = None
             class_idx = 'Oct4/profile' #options: {'Oct4/profile', 'Sox2/profile', 'Klf4/profile' or 'Nanog/profile'}
        if 1: 
            motif_A = 'GAACAATAG'
            motif_B = None
            motif_A_name = 'Sox2'
            motif_B_name = None
            max_dist = None
            class_idx = 'Sox2/profile' #options: {'Oct4/profile', 'Sox2/profile', 'Klf4/profile' or 'Nanog/profile'}
        if 0:
            motif_A = 'GGGTGTGGCC'
            motif_B = None
            motif_A_name = 'Klf4'
            motif_B_name = None
            max_dist = None
            class_idx = 'Klf4/profile' #options: {'Oct4/profile', 'Sox2/profile', 'Klf4/profile' or 'Nanog/profile'}
        if 0:
            motif_A = 'AGCCATCAA'
            motif_B = None
            motif_A_name = 'Nanog'
            motif_B_name = None
            max_dist = None
            class_idx = 'Nanog/profile' #options: {'Oct4/profile', 'Sox2/profile', 'Klf4/profile' or 'Nanog/profile'}
        if 0:
            motif_A = 'TTNNNATGCAAA'
            motif_B = None
            motif_A_name = 'Oct4-Sox2_N'
            motif_B_name = None
            max_dist = None
            class_idx = 'Oct4/profile' #options: {'Oct4/profile', 'Sox2/profile', 'Klf4/profile' or 'Nanog/profile'}

        # ========================[HARDCODED MODEL PARAMETERS]======================== #    
        
        log2FC = False
        rank_type = 'ISM' #currently the only available option provided here for BPNet
        comparison_methods = ['ISM_single', 'deepLIFT']
        alphabet = ['A','C','G','T']
        alpha = 'dna'
        bin_res = 1
        output_skip = 0
    
        userDir = os.path.join(pyDir, 'examples_%s' % example) #must already exist, with folder 'a_model_assets' propagated
        sys.path.append(os.path.join(userDir,'a_model_assets/scripts'))
        model_name = 'model_BPNet_OSKN'
        if load_model is True: #retrieve trained deep learning model
            if 0: #kipoi model (identical to below)
                import kipoi
                model = kipoi.get_model('BPNet-OSKN')
            else: #trained model from https://github.com/kundajelab/bpnet-manuscript/bpnet-manuscript-data/output/..
                  #../nexus,peaks,OSNK,0,10,1,FALSE,same,0.5,64,25,0.004,9,FALSE,[1,50],TRUE/
                from basepair.seqmodel import SeqModel
                model = SeqModel.load(os.path.join(userDir, 'a_model_assets/calibrated_seqmodel.pkl'))
            """
            Inputs shape:   (n,1000,4)
            Outputs shape:  Dictionary of 4 sets of arrays, with each set having shape (1000, 2)
            """
        else:
            model = None
    
        # retrieve genomic sequences from test set
        with h5py.File(os.path.join(userDir, 'a_model_assets/bpnet_seqs_chr1-8-9.h5'), 'r') as dataset:
            X_in = np.array(dataset['X']).astype(np.float32)
                       
                    
    elif example == 'DeepSTARR':
        """
                User parameters for determing location of recognition site(s)
                Choose one via 'if 1:' and set all others to 'if 0:'
        """
        if 0:
            motif_A = 'TGACTCA'
            motif_B = None
            motif_A_name = 'AP1'
            motif_B_name = None
            max_dist = None
            class_idx = 0 #options: {0 or 1} for developmental or housekeeping enhancer activity
        if 0:
            motif_A = 'TATCGATA'
            motif_B = None
            motif_A_name = 'DRE'
            motif_B_name = None
            max_dist = None
            class_idx = 1 #options: {0 or 1} for developmental or housekeeping enhancer activity
        if 1:
            motif_A = 'AGTGTGACC'
            motif_B = None
            motif_A_name = 'Ohler1'
            motif_B_name = None
            max_dist = None
            class_idx = 1 #options: {0 or 1} for developmental or housekeeping enhancer activity
        if 0:
            motif_A = 'CAGCTG'
            motif_B = None
            motif_A_name = 'Ohler5'
            motif_B_name = None
            max_dist = None
            class_idx = 1 #options: {0 or 1} for developmental or housekeeping enhancer activity
        if 0:
            motif_A = 'AAAATACCA'
            motif_B = None
            motif_A_name = 'Ohler6'
            motif_B_name = None
            max_dist = None
            class_idx = 1 #options: {0 or 1} for developmental or housekeeping enhancer activity
            
        # ========================[HARDCODED MODEL PARAMETERS]======================== #    
        
        log2FC = False
        rank_type = 'ISM'
        comparison_methods = ['ISM_single', 'deepSHAP']
        alphabet = ['A','C','G','T']
        alpha = 'dna'
        bin_res = 1
        output_skip = 0
    
        userDir = os.path.join(pyDir, 'examples_%s' % example) #must already exist, with folder 'a_model_assets' propagated
        sys.path.append(os.path.join(userDir,'a_model_assets/scripts'))
        model_name = 'model_DeepSTARR'
        
        if load_model is True: #retrieve trained deep learning model
            import deepstarr_deepshap
            if 0: #kipoi model (identical to below)
                import kipoi
                model = kipoi.get_model('DeepSTARR')
            else: #keras model from https://github.com/bernardo-de-almeida/DeepSTARR (needed to run DeepSHAP)
                def load_model(model):
                    from keras.models import model_from_json
                    keras_model_weights = model + '.h5'
                    keras_model_json = model + '.json'
                    keras_model = model_from_json(open(keras_model_json).read())
                    keras_model.load_weights(keras_model_weights)
                    return keras_model, keras_model_weights, keras_model_json

                model, model_weights, model_json = load_model(os.path.join(userDir, 'a_model_assets/deepstarr.model'))
            """
            Inputs shape:   (n,249,4)
            Outputs shape:  (2,)
            """
        else:
            model = None
    
        # retrieve genomic sequences from test set
        with h5py.File(os.path.join(userDir, 'a_model_assets/deepstarr_data.h5'), 'r') as dataset:
            X_in = np.array(dataset['x_test']).astype(np.float32)
            if 0:
                Y_in = np.array(dataset['y_test']).astype(np.float32).transpose()
                
        
    elif example == 'TFChIP':
        motif_A = 'CCGGAA'
        motif_B = None
        motif_A_name = 'GABPA'
        motif_B_name = None
        max_dist = None
        class_idx = None
        log2FC = False

        model_trial = 23
        model_stop = 'early'#'final' #options: {'early' or 'final'} for stopping epoch; used to analyze effects of benign overfitting
            
        # ========================[HARDCODED MODEL PARAMETERS]======================== #    
        
        rank_type = 'ISM'
        comparison_methods = [''] #pre-generated in directory
        alphabet = ['A','C','G','T']
        alpha = 'dna'
        bin_res = 1
        output_skip = 0
        
        userDir = os.path.join(pyDir, 'examples_%s' % example) #must already exist, with folder 'a_model_assets' propagated
        model_name = 'model_TFChIP_trial%s_%s' % (model_trial, model_stop)
        
        # retrieve genomic sequences from test set, model weights and attributions
        from six.moves import cPickle
        with open(os.path.join(userDir, 'a_model_assets/gabpa_results.pickle'), 'rb') as fin:
            X_in = cPickle.load(fin) #X_in.shape : (100,200,4)
            results = cPickle.load(fin) #results.keys() : trials [0, 1, 2, ..., 24]
            #print(results[model_trial].keys()) #'train_auroc','val_auroc','early_stop_index','saliency_early','sg_early','sg_final','shap_early','shap_final'
            attr_ensemble = cPickle.load(fin) #attr_ensemble.keys() : shape(100,200,4) for each key ('saliency_early', ..., etc.)

        if load_model is True: #retrieve trained deep learning model
            def CNN(input_shape, output_shape=1):
                # input layer
                inputs = keras.layers.Input(shape=input_shape)
                # layer 1
                nn = keras.layers.Conv1D(filters=32, kernel_size=15, padding='same')(inputs)        
                nn = keras.layers.Activation('relu')(nn)
                nn = keras.layers.Dropout(0.1)(nn)
                nn = keras.layers.MaxPool1D(pool_size=4)(nn)
                # layer 3
                nn = keras.layers.Conv1D(filters=64, kernel_size=5, padding='same')(nn)        
                nn = keras.layers.Activation('relu')(nn)
                nn = keras.layers.Dropout(0.2)(nn)
                nn = keras.layers.MaxPool1D(pool_size=4)(nn)
                # layer 4
                nn = keras.layers.Conv1D(filters=96, kernel_size=5, padding='same')(nn)        
                nn = keras.layers.Activation('relu')(nn)
                nn = keras.layers.Dropout(0.3)(nn)
                nn = keras.layers.MaxPool1D(pool_size=4)(nn)
                # layer 5
                nn = keras.layers.Flatten()(nn)
                nn = keras.layers.Dense(128)(nn)
                nn = keras.layers.Activation('relu')(nn)
                nn = keras.layers.Dropout(0.5)(nn)
                # output layer 
                logits = keras.layers.Dense(1, activation='linear', use_bias=True)(nn)
                outputs = keras.layers.Activation('sigmoid')(logits)
                # compile model
                model = keras.Model(inputs=inputs, outputs=outputs)
                return model 
            
            if model_stop == 'early': #load model at early stopping
                model = CNN(input_shape=(200,4), output_shape=1)
                auroc_metric = keras.metrics.AUC(curve='ROC', name='auroc')
                model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auroc_metric])
                model.load_weights(os.path.join(userDir, 'a_model_assets/gabpa_early_23.h5'))
            elif model_stop == 'final': #load overfit model 
                model = CNN(input_shape=(200,4), output_shape=1)
                auroc_metric = keras.metrics.AUC(curve='ROC', name='auroc')
                model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auroc_metric])
                model.load_weights(os.path.join(userDir, 'a_model_assets/gabpa_final_23.h5'))
                """
                Inputs shape:   (n,200,4)
                Outputs shape:  (1,1)
                """
        else:
            model = None
        

    elif example == 'CAGI5-GOPHER': #same model as used in {example == 'GOPHER'} above
        """
            Cell lines corresponding to each locus:
            https://kircherlab.bihealth.org/satMutMPRA/
        """
        motif_A = ''#'A' #disregard
        motif_B = ''#'A' #disregard
        motif_A_name = example
        motif_B_name = None #disregard
        max_dist = None #disregard
        class_idx = 0  #unused since class average taken
        log2FC = False#True
        bin_res = 32
        output_skip = 0

        if 1:
            model_name = 'model_ResidualBind32_ReLU_single'
        if 0: 
            model_name = 'model_ResidualBind32_Exp_single'
        if 0:
            model_name = 'model_Basenji32_GELU_single'

        rank_type = 'saliency'
        comparison_methods = ['ISM_single', 'saliency']
        alphabet = ['A','C','G','T']
        alpha = 'dna'

        gopherDir = os.path.join(pyDir, 'examples_GOPHER')
        userDir = os.path.join(pyDir, 'examples_CAGI5') #must already exist, with folder 'a_model_assets' propagated
        sys.path.append(os.path.join(gopherDir,'a_model_assets/scripts'))
        import saliency_embed, utils
    
        model_dir = os.path.join(gopherDir,'a_model_assets/%s' % model_name)
        if load_model is True: #retrieve trained deep learning model
            model, bin_size = utils.read_model(os.path.join(gopherDir,'a_model_assets/%s/' % model_name), compile_model = True)
        else:
            model = None

        # retrieve genomic sequences from test set
        with h5py.File(os.path.join(userDir, 'a_model_assets/CAGI5_onehots_centered_2048.h5'), 'r') as dataset:
            X_in = np.array(dataset['reference']).astype(np.float32) #shape : (15, 2048, 4)


    elif example == 'CAGI5-ENFORMER':
        """
            Cell lines corresponding to each locus:
            https://kircherlab.bihealth.org/satMutMPRA/
        """
        motif_A = ''#'A' #disregard
        motif_B = ''#'A' #disregard
        motif_A_name = example
        motif_B_name = None #disregard
        max_dist = None #disregard
        method = 'agnostic' #options: cell-type {'agnostic' or 'matched'} summary statistic
        assay = 'DNASE' #options: {'DNASE' or 'CAGE'}
        
        if assay == 'DNASE':
            log2FC = False
        elif assay == 'CAGE':
            log2FC = True

        bin_res = 128
        output_skip = int(((393216-114688)/2)/128) #see paper; re: use of 114688
        model_name = 'model_ENFORMER_%s_%s' % (method, assay)
        rank_type = 'saliency'
        comparison_methods = ['ISM_single', 'saliency']
        alphabet = ['A','C','G','T']
        alpha = 'dna'

        enformerDir = os.path.join(pyDir, 'examples_ENFORMER')
        userDir = os.path.join(pyDir, 'examples_CAGI5') #must already exist, with folder 'a_model_assets' propagated
        model_dir = os.path.join(enformerDir, 'a_model_assets')
        if load_model is True: #retrieve trained deep learning model
            model = tf.saved_model.load(model_dir).model
        else:
            model = None
        """
        Inputs shape:   (n,393216,4)
        Outputs shape:  Dictionary of 2 sets of arrays, with each set having shape (896, 5313)
        """

        gopherDir = os.path.join(pyDir, 'examples_GOPHER')
        sys.path.append(os.path.join(gopherDir,'a_model_assets/scripts'))
        import saliency_embed

        import pandas as pd
        table = pd.read_csv(os.path.join(enformerDir, 'a_model_assets/suppl_table2.csv'))
        if method == 'agnostic':
            class_idx = table.index[table['assay_type']==assay].tolist()
        else:
            print('TO BE DONE.')

        # retrieve genomic sequences from test set
        with h5py.File(os.path.join(userDir, 'a_model_assets/CAGI5_onehots_centered_393216.h5'), 'r') as dataset:
            X_in = np.array(dataset['reference']).astype(np.float32) #shape : (15, 393216, 4)        


    elif example == 'custom':
        print('TBD')
        
    else:
        print("Error: user input '%s' is not recognized." % example)
        
    return (GPU, example, motif_A, motif_B, motif_A_name, motif_B_name,
            max_muts, max_dist, rank_type, comparison_methods,
            model_name, class_idx, log2FC, alphabet, alpha,
            bin_res, output_skip, model, X_in)


def set_params_2(example):
    """
    Additional parameters required for '2_generate_mave.py' and used in subsequent scripts
    ----------

    Returns
    -------
    num_sim : INT >> 1000
        Number of mutated sequences to generate for MAVE dataset
    pred_transform : STRING {'sum', 'max', 'pca' or None}
        'sum':      Scalar y is the integral over the prediction
        'max':      Scalar y is the max value of the prediction
        'pca':      Scalar y is defined by the first principal component; generated by PCA over all predictions
        None:       Set to None if model predictions are already in single-value (scalar) format per sequence
    pred_trans_delimit : INT >= 0 or None
        Delimit range of values taken during above transformation; e.g., if 'sum' is chosen above,
        a value of 5 will sum over five flanking units on each side of the sequence's center position.
        Units correspond to the prediction output, not the input (e.g., bins, not base pairs)
    delimit_start : INT
        Specify start position for range of 'pred_trans_delimit'
    delimit_stop : INT
        Specify stop position for range of 'pred_trans_delimit'
    scope : STRING {'intra', 'inter'}
        Define scale at which to probe each recognition site instance. Choosing 'intra' will load in the..
        ..dataframe containing all single recognition-site instances (e.g., 'AP1_positions.csv'), while..
        choosing 'inter' will instead load in the dataframe containing all pairs of recognition-site..
        ..instances (e.g., 'AP1_AP1_positions.csv'). Each of these dataframes must be previously generated..
        ..by the script 1_locate_patterns.py if not already provided in our example folders
        'intra':    Probe region of sequence that includes a recognition site of interest
        'inter':    Probe region of sequence that includes two recognition sites of interest
    sort : BOOL {True or False}
        If True, sort 'intra' or 'inter' dataframe (see above) by a desired condition(s) to select specific..
        ..types of sequences on which to apply surrogate modeling. The default sorting (True) uses the 'motif rank'..
        ..column of the 'intra' or 'inter' dataframe. The variable 'df_idx' in '2_generate_mave_batch.py' and..
        ..'3_surrogate_modeling.py' corresponds to each sequence ordered by this sorting. As seen in the..
        ..'if sort is True:' section of these scripts, this sorting also accounts for pruning based on total..
        ..number of core recognition-site mutations (see 'use_mut' description below) and positional bias..
        ..and can be further customized as desired.
        (automatically disabled for example='TFChIP')
    use_mut : 0 <= INT <= 'max_muts'
        If 'sort' is True, set total number of core mutations of recognition sites used during surrogate modeling.
        For example, if {use_mut == 0}, only recongition-site instances with zero core mutations will be..
        ..considered as candidates for surrogate modeling
    model_pad : INT >= 0 (within length of sequence) or 'full' (STRING)
        Option to add extra flanking bp positions around core motif(s) to establish an extended sequence region..
        ..for surrogate modeling (and prior to that, on which to apply mutagenesis). Setting 'model_pad' to..
        ..'full' will probe the entire sequence [0, L]. Due to memory constraints when using pairwise terms for..
        ..surrogate modeling (see 'gpmap' in set_params_3()), 'full' is only recommended for additive models
    compare : BOOL {True or False}
        If True, generate standard attribution maps for comparison with surrogate modeling. Available attribution..
        ..maps are predefined for each deep learning model via the 'comparison_methods' variable above
        (automatically disabled for example='TFChIP')
    map_crop : BOOL {True or False}
        If True, crop single ISM attribution map within same delimited region used for surrogate modeling.
        The range of delimited region is defined by 'model_pad'. Delimiting will increase computation speed but..
        ..restrict visualization of the whole sequence to only the targeted region. Additionally to consider, the..
        ..accuracy of PCA (if defined via 'pred_transform') may be affected for small 'model_pad' values
    max_in_mem : {INT > 0 or False}
        Total number of inputs allowed in one call for model to predict; based on system memory
        Supplying a positive integer sets the batch size for predictions, which are allocated to disk via memmap
        (See WARNING in ink.py : calamari() about required disk space before proceeding)
        If False, no batches are used for model predictions and raw predictions are not saved to disk
    clear_RAM : BOOL {True or False}
        If True, intermittently clear up memory where appropriate
    save : BOOL {True or False}
        If True, save all figures to user directory
    """

    num_sim = 100000
    if example == 'GOPHER':
        pred_transform = 'sum'
        pred_trans_delimit = None
        model_pad = 100 #used for all scope='intra' studies
        #model_pad = 6 #used for AP-1/AP-1 pairwise (scope='inter'); also 'compare'=False
        max_in_mem = False
        sort = True
    elif example == 'BPNet':
        pred_transform = 'wn'
        pred_trans_delimit = None#100
        model_pad = 100
        max_in_mem = False
        sort = True
    elif example == 'CAGI5-GOPHER':
        pred_transform = 'sum'
        pred_trans_delimit = None
        model_pad = 400 #options: {INT > 300 or None}; if None, exactly encloses locus of interest without boundaries
        max_in_mem = False
        sort = False
    elif example == 'CAGI5-ENFORMER':
        pred_transform = 'sum'
        pred_trans_delimit = 5 #number of 128-bp wide bins on each side of central position
        model_pad = 400 #options: {INT > 300 or None}; if None, exactly encloses locus of interest without boundaries
        max_in_mem = 5
        sort = False
    elif example == 'DeepSTARR' or example == 'TFChIP':
        pred_transform = None
        pred_trans_delimit = None
        model_pad = 'full'
        max_in_mem = False
        sort = True
    use_mut = 0
    scope = 'intra'
    compare = True
    map_crop = True
    clear_RAM = True
    save = True
    
    return (num_sim, pred_transform, pred_trans_delimit, scope, sort, use_mut, model_pad, compare, map_crop, max_in_mem, clear_RAM, save)


def set_params_3():
    """
    Additional parameters required for '3_surrogate_modeling.py' and used in subsequent scripts
    ----------

    Returns
    -------
    surrogate : STRING {'mavenn' or 'ridge'}
        Type of surrogate model to use
            'mavenn':   use the MAVE-NN suite (recommended)
            'lasso':    lasso regression (scikit-learn Lasso)
            'ridge':    ridge regression (scikit-learn RidgeCV)
            'lime':     least squares linear regression on k features
    regression : STRING {'MPA' or 'GE'}
        Type of regression used for measurement process
            'MPA':      measurement process agnostic (categorical y-values)
            'GE':       global epistasis (continuous y-values)
    gpmap : STRING {'additive' or 'pairwise'}
        If using MAVE-NN: define surrogate model used to interpret deep learning model
        More options will be introduced in the future
    gauge : STRING {'uniform', 'empirical', 'wildtype', 'consensus' or 'user'}
        Allowed values of which gauge to use:
        'uniform':      Hierarchical gauge using a uniform sequence distribution over the characters at each..
                        ..position observed in the training set (unobserved characters are assigned probability 0)
        'empirical':    Hierarchical gauge using an empirical distribution computed from the training data
        'consensus':    Wild-type gauge using the training data consensus sequence
        'user':         Gauge using either p_lc or x_wt supplied by the user
        'none':         No gauge fixing
    linearity : STRING {'linear' or 'nonlinear'}
        If using MAVE-NN: linearity of measurement process
    noise : STRING {'Gaussian', 'Cauchy' or 'SkewedT'}
        If using MAVE-NN: noise models for GE model context (see 'regression' above)
    noise_order : INT >= 0
        If using MAVE-NN: in the GE model context, represents the order of the polynomial(s) used to define noise model parameters
    drop : BOOL {True or False}
        If True, drop all duplicate {x, y} rows in MAVE dataframe
    """

    surrogate = 'mavenn' #options: {'mavenn', 'lasso', 'ridge', 'lime'}
    drop = True

    if surrogate == 'mavenn': #see MAVE-NN documentation for more information on these parameters and others
        regression = 'GE'
        gpmap = 'additive'#'pairwise'
        gauge = 'empirical' #use 'empirical' for default MAVE-NN gauge
        if 1:
            linearity = 'nonlinear'
            noise_order = 2
        else:
            linearity = 'linear'
            noise_order = 0
        noise = 'SkewedT'
    elif surrogate == 'ridge' or surrogate == 'lasso' or surrogate == 'lime':
        regression, gpmap, gauge, linearity, noise_order, noise = None, None, None, None, None, None

    else:
        print("Error: user input '%s' is not recognized." % surrogate)

    return (surrogate, regression, gpmap, gauge, linearity, noise, noise_order, drop)


def set_params_4():
    """
    Additional parameters required for '4_analyze_outputs.py'
    ----------

    Returns
    -------
    seq_total : 1 < INT < T
        Total number of sequences modeled in the previous script ('T') or a subset
    gauge : STRING {'wildtype', 'hierarchical', 'default', 'empirical'}
        Specification of which gauge to use for fixing attribution matrix parameters;
        'empirical' is the same as 'hierarchical' with the exception that SQUID attribution maps..
        are transformed into the 'hierarchical' gauge empirically using its training data
    show_cropped : BOOL {True or False}
        If True, save a figure for each of the cropped elements for all intermediate sequences leading up to averaging
        Specifically, from each model calculated per sequence in the previous script, crop out motif A and motif B..
        ..terms in the additive model, as well as motif A and B and their interaction (I) terms in the corresponding..
        ..pairwise model (if applicable). These cropped instances are then aligned and averaged
    show_compared : BOOL {True or False}
        If True, save a range of page-long figures featuring close-ups of motif comparisons for attribution-based methods
    fig_pad : 0 <= INT <= 'model_pad'
        If nonzero, include additional flanking bp positions (e.g., 'fig_pad' flanks per each side of 'motif_A')
        These additional flanks will be displayed in all figures and used when calculating ensemble statistics
    show_compared : BOOL {True or False}
        If True, filter out all instances where {use_mut != the INT defined above}
        Only applicable when {scope = inter}; e.g., if 'motif_A' has 1 core mutation while 'motif_B' has no core mutations..
        ..and if {use_mut == 0}, then 'motif_A' will be ignored when calculating ensemble statistics
    standardize_local : BOOL {True or False}
        If True, standardize each attribution map individually regardless of other attribution maps in the ensemble
    standardize_global : BOOL {True or False}
        If True, standardize attribution maps across the entire ensemble
        To note, the combination {standardize_local = False and standardize_global = True} does not provide a fair comparison..
        ..of the different attribution methods, since mavenn outputs arrive normalized while maps from the other methods do not
    """

    seq_total = 50
    gauge = 'hierarchical' #if running via '4_batch.sh', a list of gauges are supplied which will overwrite this argument
    show_cropped = False
    show_compared = False
    filter_muts = False
    standardize_local = True
    standardize_global = False #recommended to keep False (see comments in '4_analyze_outputs.py')
    fig_pad = None #see comment below
    '''
        'fig_pad' is set as an additional argument when running the Python script in CLI; e.g., 
                'python 4_analyze_outputs.py 15'
            ..or a list of 'fig_pads' can be supplied to '4_batch.sh' to process in a loop; then running:
                'bash 4_batch.sh'
    '''
    
    return (seq_total, gauge, show_cropped, show_compared, fig_pad, filter_muts, standardize_local, standardize_global)
    


"""
Model parameters used in our example analysis:

num_sim = 100000

GOPHER:
- pred_transform = 'pca'
- scope = 'intra'
	- model_pad = 100 #for additive
- scope = 'inter'
	- model_pad = 6 #for pairwise

BPNet:
- pred_transform = 'pca'
- scope = 'intra'
	- model_pad = 100 #for additive
- scope = 'inter'
	- model_pad = TBD #for pairwise

DeepSTARR:
- pred_transform = None
- scope = 'intra'
	- model_pad = 'full' #for additive
- scope = 'inter'
	- model_pad = TBD #for pairwise
"""
    