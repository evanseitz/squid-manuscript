import os, sys
sys.dont_write_bytecode = True
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import pandas as pd
import logomaker

# =============================================================================
# TO DO:
    # Finish updating readme comments
    # Rework ISM_double --> needs to include l2_norm
# =============================================================================


def oh2seq(OH, alphabet):
    """
    Convert one-hot encoding to sequence
    ----------
    OH : ARRAY with shape (L, 4)
        Input sequence (one-hot encoding)
    alphabet : 1D ARRAY
        All characters present in the sequence alphabet (e.g., ['A','C','G','T'] for DNA)
    
    Returns
    -------
    seq : STRING with length L
        Sequence corresponding to input one-hot encoding (e.g., 'AATGAC...')
    """
    
    seq = []
    for i in range(np.shape(OH)[0]):
        for j in range(len(alphabet)):
            if OH[i][j] == 1:
                seq.append(alphabet[j])
    seq = ''.join(seq)
    return seq


def seq2oh(seq, alphabet):
    """
    Convert sequence to one-hot encoding
    ----------
    seq : STRING with length L
        Input sequence
    alphabet : 1D ARRAY
        All characters present in the sequence alphabet (e.g., ['A','C','G','T'] for DNA)
    
    Returns
    -------
    OH : ARRAY with shape (L, 4)
        One-hot encoding corresponding to input sequence
    """
    
    L = len(seq)
    OH = np.zeros(shape=(L,len(alphabet)), dtype=np.float32)
    for idx, i in enumerate(seq):
        for jdx, j in enumerate(alphabet):
            if i == j:
                OH[idx,jdx] = 1
    return OH


def ISM_single(x, model, class_idx, example, get_prediction, unwrap_prediction, compress_prediction,
               pred_transform, pred_trans_delimit, log2FC, max_in_mem, save, save_dir,
               start=None, stop=None, delimit_start=None, delimit_stop=None):
    """
    Single ISM for a given sequence
    ----------
    x : ARRAY with shape (L, 4)
        input sequence (one-hot encoding)
    model : FUNCTION (model dependent)
        User-defined deep learning model
    class_idx : INT or STRING (model dependent)
        The indexing scheme for obtaining a desired prediction property from the deep learning model
    example : STRING {'GOPHER', 'DeepSTAR', 'BPNet' or 'custom'}
        Moniker for assigning functions in 'unwrap_prediction()' and 'compress_prediction()'
    unwrap_prediction : FUNCTION
        Customizable function required for reading a specific deep learning model prediction
    compress_prediction : FUNCTION
        Customizable function required for compressing a targeted deep learning model prediction into..
        a single scalar value. Targeted model predictions are first isolated via 'unwrap_predictions()'
    pred_transform : STRING {'sum', 'max', 'pca', 'custom' or None}
        'sum':      Scalar y is the integral over the prediction
        'max':      Scalar y is the max value of the prediction
        'pca':      Scalar y is defined by the first principal component; generated by PCA over all predictions
        'custom':   Users may also define their own function within 'compress_prediction()'
        None:       Set to None if model predictions are already in single-value (scalar) format per sequence
    pred_trans_delimit : INT >= 0 or None
        Delimit range of values taken during above transformation; e.g., if 'sum' is chosen above,
        a value of 5 will sum over five flanking units on each side of the sequence's center position.
        Units correspond to the prediction output, not the input (e.g., bins, not base pairs)
    delimit_start : INT
        Specify start position for range of 'pred_trans_delimit'
    delimit_stop : INT
        Specify stop position for range of 'pred_trans_delimit'
    log2FC : BOOL {True or False}
        If True, calculate ISM predictions via the log2 fold-change {log2(pred_var) - log2(pred_wt)}
        If False, just take the difference {pred_var - pred_wt}
    save_dir : STRING or None
        Directory location for saving intermediate figures (optional)
    start : INT >= 0
        Delimited start position on input sequence
    stop: INT <= L
        Delimited stop position on input sequence
                    
    Returns
    -------
    score_matrix : ARRAY with shape ('stop' - 'start', 4)
        Result of ISM; score_matrix must still be normalized..
        ..across variants for each position to construct attribution map
        (see 'l2_norm_to_df' below)
    """
    
    L, A = x.shape
    
    if start is None:
        start = 0
    if stop is None:
        stop = L
            
    WT_pred_full = get_prediction(np.expand_dims(x, 0), example, model)
    WT_pred = unwrap_prediction(WT_pred_full, class_idx, 0, example, pred_transform)
    
    num_preds = (A-1)*(stop-start)
    
    if pred_transform != 'pca':
        if pred_trans_delimit is not None:
            pred_scalar_wt = compress_prediction(WT_pred, pred_transform, pred_trans_delimit)
        else:
            pred_scalar_wt = compress_prediction(WT_pred, pred_transform, pred_trans_delimit, delimit_start=delimit_start, delimit_stop=delimit_stop)
        
    one_hots = np.zeros((num_preds, L, A))
    one_hots_loc = np.zeros(num_preds)
    score_matrix = np.zeros(shape=(L, A), dtype=float)
    idx = 0
    for p in range(start,stop):
        x_mut = np.copy(x)
        for b in range(1,A):
            x_mut[p,:] = np.roll(x_mut[p,:], 1)
            B = np.where(x_mut[p,:] == 1)[0]
            one_hots[idx,:,:] = x_mut
            one_hots_loc[idx] = B
            idx += 1
                
    
    if max_in_mem is not False: #batch prediction mode
        memmap_out = os.path.join(save_dir, 'ISM_preds_raw.npy')
        mut_preds_full = np.memmap(memmap_out, mode='w+', dtype='float32', 
                                shape=(num_preds, WT_pred_full.shape[1], WT_pred_full.shape[2]))

        def partition(l, n):
            for i in range(0, len(l), n):
                yield l[i:i + n]
        batches = list(partition(np.arange(num_preds), max_in_mem))

        for batch_idx in range(len(batches)):
            print('Batch: %s of %s' % (batch_idx+1, len(batches)))
            preds = get_prediction(one_hots[batches[batch_idx]], example, model)
            mut_preds_full[batches[batch_idx]] = preds

        if 1:
            os.remove(memmap_out)

    else: #no batch predictions
        mut_preds_full = get_prediction(one_hots, example, model)

    
    mut_profiles = pd.DataFrame(columns = ['y'], index=range(num_preds))
    
    for n in range(num_preds):
        if pred_transform == 'pca':
            mut_profiles.at[n, 'y'] = unwrap_prediction(mut_preds_full, class_idx, n, example, pred_transform)
        else:
            mut_profiles.at[n, 'y'] = compress_prediction(unwrap_prediction(mut_preds_full, class_idx, n, example, pred_transform), pred_transform, pred_trans_delimit, delimit_start=delimit_start, delimit_stop=delimit_stop)
        
    del mut_preds_full

    if pred_transform == 'pca':
        pred_scalars_mut, pred_scalar_wt = pca(mut_profiles, WT_pred, save_dir, save_name='attributions_ISM_single')
        if log2FC is True:
            pred_min = pred_scalars_mut['y'].min()
            pred_scalars_mut['y'] += (abs(pred_min) + 1)
            pred_scalar_wt += (abs(pred_min) + 1)
    else:
        if log2FC is True:
            pred_scalar_wt = WT_pred
            pred_min = mut_profiles['y'].min()
            mut_profiles['y'] += (abs(pred_min) + 1)
            pred_scalar_wt += (abs(pred_min) + 1)

    # plot histogram of ISM predictions
    if save is True:
        if pred_transform == 'pca':
            plot_muts = pred_scalars_mut['y']
        else:
            plot_muts = mut_profiles['y']
        fig, ax = plt.subplots()
        ax.hist(plot_muts, bins=100)
        ax.set_xlabel('y')
        ax.set_ylabel('Frequency')
        ax.axvline(pred_scalar_wt, c='red', label='WT', linewidth=2, zorder=10)
        plt.legend(loc='upper right')
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir,'attr_distribution_ISM.png'), facecolor='w', dpi=200)
        plt.close()

    p_idx = start
    b_idx = 0
    idx = 0

    for i in range(num_preds):
        if pred_transform == 'pca':
            if log2FC is True:
                if pred_scalars_mut['y'][i] != pred_scalar_wt:
                    score_matrix[p_idx, int(one_hots_loc[i])] = np.log2(pred_scalars_mut['y'][i]) - np.log2(pred_scalar_wt)
                else:
                    score_matrix[p_idx, int(one_hots_loc[i])] = 0.
            else:
                score_matrix[p_idx, int(one_hots_loc[i])] = pred_scalars_mut['y'][i] - pred_scalar_wt
        else:
            if log2FC is True:
                if mut_profiles['y'][i] != pred_scalar_wt:
                    score_matrix[p_idx, int(one_hots_loc[i])] = np.log2(mut_profiles['y'][i]) - np.log2(pred_scalar_wt)
                else:
                    score_matrix[p_idx, int(one_hots_loc[i])] = 0.
            else:
                score_matrix[p_idx, int(one_hots_loc[i])] = mut_profiles['y'][i] - pred_scalar_wt

        if b_idx == A-2:
            p_idx += 1
        if b_idx < A-2:
            b_idx += 1
        else:
            b_idx = 0
            
    return score_matrix


def ISM_double(x, model, start, stop, model_type, class_num=None):
    """
    Double ISM for a given sequence
    
    x:          input sequence (one-hot encoding);
                    shape (L, 4)
    start:      delimited start position on original sequence;
                    positive integer
    stop:       delimited stop position on original sequence;
                    positive integer
    class_num:  index of prediction class read from model;
                    positive integer
                    
    Before use, review get_score() for defining custom score based on user model
    """
    
    mut_len = stop - start
    ISM_matrix = np.zeros(shape=(mut_len,4,mut_len,4))
    wt_score = get_score(np.expand_dims(x, 0), model, model_type, class_num)
    
    for i in range(start, stop):
        print('Position: %s of %s' % (i,stop-1))
        for j in range(start, stop):
            if i < j:
                for k in range(0,4):
                    x_mut1 = np.array(x, copy=True)
                    x_mut1[i,:] = 0
                    x_mut1[i,k] = 1
                    for l in range(0,4):
                        x_mut2 = np.array(x_mut1, copy=True)
                        x_mut2[j,:] = 0
                        x_mut2[j,l] = 1
                        
                        prediction = get_score(np.expand_dims(x_mut2, 0), model, model_type, class_num) #ZULU
                        ISM_matrix[i-start,k,j-start,l] = (prediction - wt_score)     
    
    return ISM_matrix


def pca(mave, WT, save_dir, save_name=None):
    mave_pca = mave.copy()
    
    # create array of sums for eventual reference (i.e., sense correction)
    mave_sum = mave.copy()
    mave_sum['y'] = mave_sum['y'].apply(lambda x: np.sum(x))

    N = mave.shape[0] + 1
    B = mave['y'][0].shape[0]
    
    #Y = np.stack(mave['y'])
    Y = np.ndarray(shape=(N, B))
    for i in range(0,N):
        if i < (N-1):
            Y[i,:] = mave['y'][i]
        else:
            Y[i,:] = WT
    
    # mean of all distributions is subtracted from each distribution
    mean_all = np.mean(Y, axis=0) #dimension = number of bins
    for i in range(N):
        Y[i,:] -= mean_all

    print('Computing SVD...')
    u,s,v = np.linalg.svd(Y.T, full_matrices=False)
    vals = s**2 #eigenvalues
    vecs = u #eigenvectors
    print('SVD complete')
    
    U = Y.dot(vecs)
    v1, v2 = 0, 1
    
    corr = np.corrcoef(mave_sum['y'], U[:,v1][:-1])
    if corr[0,1] < 0: #correct for eigenvector "sense"
        U[:,v1] = -1.*U[:,v1]
    mave_pca['y'] = U[:,v1][:-1]
    
    if save_name is not None and save_dir is not None:
        # view eigenvalue spectrum
        x = range(1,len(vals)+1)
        plt.scatter(x, vals)
        plt.title('Eigenvalue spectrum')
        plt.xlabel(r'$PC$')
        plt.ylabel(r'$\mathrm{\lambda}$', rotation=0)
        plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))
        plt.xlim([0,15])
        plt.ylim(-vals[0]/8., vals[0]+vals[0]/8.)
        plt.locator_params(nbins=15)
        plt.axhline(y=0, color='k', alpha=.5, linestyle='--', linewidth=1)
        plt.savefig(os.path.join(save_dir,'%s_pca_eigvals.png' % save_name), facecolor='w', dpi=200)
        plt.close()
        
        # view leading 2D subspace
        fig, ax = plt.subplots()
        ax.scatter(U[:,v1], U[:,v2], s=1, facecolor='k', alpha=.5)
        ax.scatter(U[:,v1][-1], U[:,v2][-1], s=5, facecolor='r', label='WT')
        ax.set_xlabel(r'$PC_%s$' % (v1+1), fontsize=20)
        ax.set_ylabel(r'$PC_%s$' % (v2+1), fontsize=20)
        plt.legend(loc='best')
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir,'%s_pca_eigvecs.png' % save_name), facecolor='w', dpi=200)
        plt.close()
         
    return (mave_pca, U[:,v1][-1])


def arr2pd(x, alphabet):
    """
    Convert Numpy array to Pandas dataframe with proper column headings
    ----------
    x : ARRAY with shape (L, 4)
        input sequence (one-hot encoding or attribution map)
    alphabet : 1D ARRAY
        All characters present in the sequence alphabet (e.g., ['A','C','G','T'] for DNA)
    
    Returns
    -------
    x : DATAFRAME
        Pandas dataframe corresponding to the input Numpy array
    """
    
    labels = {}
    idx = 0
    for i in alphabet:
        labels[i] = x[:,idx]
        idx += 1
    x = pd.DataFrame.from_dict(labels, orient='index').T
    
    return x


def rescale(x, a, b):
    """
    Rescale and normalize a given matrix
    
    x:          input matrix
    a:          new lower bound; positive integer
    b:          new upper bound; positive integer
                
    """
    x = (b-a) * ((x-x.min()) / (x.max()-x.min())) + a
    
    return x


def normalize(x, x_full): #technically a standardization
    std = x_full.std() #not position dependent
    mu = np.expand_dims(np.mean(x, axis=1), axis=1) #position dependent
    z = (x - mu) / std
    
    return z


def l2_norm_to_df(x, scores, alphabet, alpha):
    """    
    Generate pandas dataframe for saliency plot..
    based on l2-norm of scores (i.e. ISM)
    
    Adapted from TFomics/impress.py
    
    """
    alphabet = ''.join(alphabet)
    x = np.squeeze(x)
    x_index = np.argmax(x, axis=1)
    scores = np.squeeze(scores)
    L, A = scores.shape
      
    # calculate l2-norm
    scores = np.sqrt(np.sum(scores**2, axis=1))# + 1e-10)
      
    # create dataframe
    seq = ''
    saliency = np.zeros((L))
    for i in range(L):
        seq += alphabet[x_index[i]]
        saliency[i] = scores[i]
      
    # create saliency matrix
    saliency_df = logomaker.saliency_to_matrix(seq=seq, values=saliency, alphabet=alpha)
      
    return saliency_df


def fix_gauge(x, gauge, wt=None):
    """    
    Fix the gauge for an attribution matrix
    
    x :         ARRAY with shape (L, 4)
                Matrix of attribution scores for a sequence-of-interest
    gauge :     STRING {'empirical', 'wildtype', 'hierarchical', 'default'}
                Specification of which gauge to use
    OH_wt :     ARRAY with shape (L, 4)
                Wild-type sequence (one-hot encoding); needed if gauge = 'wildtype'

    """

    x1 = x.copy()

    if gauge == 'empirical':
        r = 0.10 #probability of mutation
        L = wt.shape[0] #length of sequence
        wt_argmax = np.argmax(wt, axis=1) #index of each wild-type in the one-hot encoding

        p_lc = np.ones(shape=wt.shape) #empirical probability matrix
        p_lc = p_lc*(r/3.)

        for l in range(L):
            p_lc[l,wt_argmax[l]] = (1-r)

        for l in range(L):
            weighted_avg = np.average(x[l,:], weights=p_lc[l,:])
            for c in range(4):
                x1[l,c] -= weighted_avg

    elif gauge == 'wildtype':
        L = wt.shape[0]
        wt_argmax = np.argmax(wt, axis=1)
        for l in range(L):
            wt_val = x[l, wt_argmax[l]]
            x1[l,:] -= wt_val

    elif gauge == 'hierarchical':
        for l in range(x.shape[0]):
            col_mean = np.mean(x[l,:])
            x1[l,:] -= col_mean

    elif gauge == 'default':
        pass

    return x1